{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split \n",
    "from numpy import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import math\n",
    "\n",
    "def get_data():\n",
    "    data = load_svmlight_file(\"F:\\\\BaiduNetdiskDownload\\\\machinelearning\\\\a9a.txt\")\n",
    "    return data[0], data[1]\n",
    "\n",
    "def function(x,w):\n",
    "    z = 0 \n",
    "    x = np.ndarray.tolist(x)\n",
    "#    w = np.ndarray.tolist(w)\n",
    "#    print(x)\n",
    "#    print(w)\n",
    "#    print(x[0][0])\n",
    "#    print(len(x))\n",
    "    for i in range(len(x)):\n",
    "        z += x[0][i] * w[i] \n",
    "    f = 1/(1+exp(-z))\n",
    "#    print f\n",
    "    return f\n",
    "\n",
    "x,y=get_data()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.33,random_state=42)\n",
    "#b = random.random() #random\n",
    "w = random.random(size=(123,1))\n",
    "#print(w)\n",
    "\n",
    "count = 300\n",
    "lr=0.01\n",
    "feature=123\n",
    "x_train = x_train.todense()\n",
    "x_test = x_test.todense()\n",
    "x_train_len=len(x_train)\n",
    "x_test_len=len(x_test)\n",
    "train_new_loss = []\n",
    "test_new_loss = []\n",
    "h = function(x_train[0],w)\n",
    "#print(h)\n",
    "#print(y_train[i] * math.log(function(x_train[i],w)) + (1 - y_train[i]) * math.log(function(x_train[i],w)) for i in range(x_train_len))\n",
    "loss = -1*(sum(((1+y_train[i]) * math.log(function(x_train[i],w)) + (1 - y_train[i]) * math.log(1-function(x_train[i],w))) for i in range(x_train_len)))/x_train_len\n",
    "#print(loss)\n",
    "for k in range(count):\n",
    "#    print(w)\n",
    "#    print(x_train_len)\n",
    "    i = random.randint(x_train_len)\n",
    "    temp = np.ndarray.tolist((function(x_train[i],w) - y_train[i]) * x_train[i])\n",
    "#    print(temp)\n",
    "    for j in range(feature):\n",
    "        w[j] = w[j] - lr * temp[0][j]\n",
    "#    print(w)\n",
    "#    print(w)\n",
    "#    print(y_train[i])\n",
    "    train_loss = -1*(sum(((1+y_train[i]) * math.log(function(x_train[i],w)) + (1 - y_train[i]) * math.log(1-function(x_train[i],w))) for i in range(x_train_len)))/x_train_len\n",
    "    test_loss = -1*(sum(((1+y_test[i]) * math.log(function(x_test[i],w)) + (1 - y_test[i]) * math.log(1-function(x_test[i],w))) for i in range(x_test_len)))/x_test_len\n",
    "\n",
    "#    if (loss - train_loss) > 0:\n",
    "#    loss = train_loss\n",
    "    train_new_loss.append(train_loss)\n",
    "    test_new_loss.append(test_loss)\n",
    "#    else:\n",
    "#        break\n",
    "# print(train_new_loss)\n",
    "# print(test_new_loss)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.xlabel('Stochastic Gradient descent - Iteration times')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(range(count), train_new_loss, 'o-', label=u\"Training Set\")\n",
    "plt.plot(range(count), test_new_loss, 'r-', label=u\"Testing set\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
